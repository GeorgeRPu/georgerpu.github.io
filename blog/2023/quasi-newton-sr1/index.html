<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Quasi-Newton Methods (1/2): SR1 | George Pu</title> <meta name="author" content="George Pu"> <meta name="description" content="An introduction to quasi-Newton methods."> <meta name="keywords" content="academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%AE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://georgerpu.github.io/blog/2023/quasi-newton-sr1/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">George </span>Pu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Quasi-Newton Methods (1/2): SR1</h1> <p class="post-meta">February 14, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/mathematics"> <i class="fa-solid fa-hashtag fa-sm"></i> mathematics</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>Newton’s method</strong> is a classic optimization algorithm. Given a function $f\colon \mathbb{R}^n \to \mathbb{R}$ which we want to minimize, Newton’s method updates a starting point $x_0$ based on the gradient $\nabla f$ and Hessian $Hf$.</p> \[x_{k+1} = x - [Hf(x_k)]^{-1} \nabla f(x_k)\] <p>While the gradient $\nabla f(x)$ is a $n$-dimensional vector of the partial derivatives</p> \[\left(\frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_n} \right),\] <p>the Hessian is a $n \times n$​ matrix of all second derivatives.</p> \[\begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} &amp; \dots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \dots &amp; \frac{\partial^2 f}{\partial x_n^2} \end{bmatrix}\] <p>Because</p> <ol> <li>the Hessian scales quadratically with the number of dimensions and</li> <li>must be inverted,</li> </ol> <p>for applications with high dimensions, such as optimizing the millions of parameters of a neural network, <strong>quasi-Newton methods</strong> that approximate the inverse Hessian from the gradient are preferred over Newton’s method. We try to approximate the inverse Hessian instead of the Hessian to avoid performing an expensive matrix inverse.</p> <p>Let $H_k = B_k^{-1}$ be a matrix. Here, $B_k$ approximates the Hessian while $H_k$​​ approximates the Hessian’s inverse.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> Our quasi-Newton update rule takes the form:</p> \[x_{k+1} = x_k - H_k \nabla f(x_k).\] <p>Let $s_k = x_{k+1} - x_k$. Then</p> \[s_k = - H_k \nabla f(x_k).\] <p>So long as $H_k$ is positive definite, then</p> \[-f(x_k)^\top s_k = -f(x_k)^\top H_k \nabla f(x_k).\] <p>The directional derivative $\nabla f(x_k)^\top s_k$ is negative and so $s_k$​ is in the downhill direction. Note that if $H_k = I$, the equation above reduces to gradient descent, and if $H_k = [Hf(x)]^{-1}$, the equation reduces to Newton’s method.</p> <p>There are many different approaches to approximate $H_k$​​​​ for quasi-Newton methods. We present 3 over 2 posts.</p> <h2 id="symmetric-rank-1">Symmetric Rank 1</h2> <p>We can approximate $H_k$ using the secant equation. The secant equation is the multidimensional generalization of the 1D rule that the derivative is approximately the slope of the secant line:</p> \[f'(x_1) \approx \frac{f'(x_1) - f''(x_0)}{x_1 - x_0}.\] <p>Let $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$ and $s_k = x_{k+1} - x_k$ as before.</p> \[B_{k+1}s_k = y_k\] <p>The secant equation gives us $n$ linear equations but $n(n + 1)/2$ unknown values in $B_{k+1}$. The secant equation is underdetermined.</p> <p>To compensate, we make the ansatz that the $B_{k+1}$ update takes the form</p> \[B_{k+1} = B_k + \sigma vv^\top\] <p>where $\sigma = \pm 1$. This method is called <strong>Symmetric Rank 1</strong> because $vv^\top$​ is a symmetric rank 1 matrix.</p> <p>Let’s apply the ansatz to the secant equation.</p> \[\begin{align*} y_k &amp;= B_{k+1}s_k \\ &amp;= B_ks_k + \sigma vv^\top s_k \\ &amp;= B_ks_k + (\sigma v^\top s_k)v \end{align*}\] <p>Solve for $v$​.</p> \[\begin{align*} y_k - B_ks_k &amp;= (\sigma v^\top s_k)v \\ v &amp;= \frac{y_k - B_ks_k}{\sigma v^\top s_k} \end{align*}\] <p>From the equation for $v$, we get</p> \[\begin{align*} v^\top s_k &amp;= \frac{(y_k - B_ks_k)^\top s_k}{\sigma v^\top s_k} \\ (v^\top s_k)^2 &amp;= \frac{(y_k - B_ks_k)^\top s_k}{\sigma} \end{align*}\] <p>Now we can eliminate $vv^\top$ and $\sigma$​​​ from the update.</p> \[\begin{align*} \sigma vv^\top &amp;= \sigma \frac{y_k - B_ks_k}{\sigma v^\top s_k} \cdot \frac{(y_k - B_ks_k)^\top}{\sigma v^\top s_k} \\ &amp;= \sigma \frac{(y_k - B_ks_k)(y_k - B_ks_k)^\top}{\sigma^2 (v^\top s_k)^2} \\ &amp;= \sigma \frac{(y_k - B_ks_k)(y_k - B_ks_k)^\top}{\frac{\sigma^2}{\sigma}(y_k - B_ks_k)^\top s_k} \\ &amp;= \frac{(y_k - B_ks_k)(y_k - B_ks_k)^\top}{(y_k - B_ks_k)^\top s_k} \\ \end{align*}\] <p>After substituting the equation for $\sigma vv^\top$ back into the update rule for $B_k$, we obtain</p> \[B_{k+1} = B_k + \frac{(y_k - B_ks_k)(y_k - B_ks_k)^\top}{(y_k - B_ks_k)^\top s_k}.\] <p>We can use the <a href="https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula" rel="external nofollow noopener" target="_blank">Sherman-Morrison formula</a></p> \[(A + uv^\top)^{-1} = A^{-1} - \frac{A^{-1}uv^\top A^{-1}}{1 + v^\top A^{-1}u}\] <p>to calculate the update for the inverse Hessian approximation $H_k$.</p> \[H_{k+1} = H_k + \frac{(s_k - H_k y_k)(s_k - H_k y_k)^\top}{(s_k - H_k y_k)^\top y_k}\] <h3 id="pros">Pros</h3> <ul> <li>Storing and updating $H_k$ is $O(n^2)$.</li> <li>The outer product $vv^\top$ is positive semi-definite but not necessarily positive definite, making SR1 a good quadratic approximation for non-convex optimization problems.</li> </ul> <h3 id="cons">Cons</h3> <ul> <li>The denominator can be 0 if $s_k = H_k y_k$ or $(s_k - H_k y_k)^\top y_k = 0$. The only solution is to skip the SR1 update: $H_{k+1} = H_k$​.</li> </ul> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>Yes, it is confusing to have $H_k$ approximate the inverse of the Hessian, but this is the standard notation. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/quasi-netwon-bfgs/">Quasi-Newton Methods (2/2): BFGS</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/books/">2022 in Books</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/crdt/">Conflict Free Replicated Data Types</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/exp-inflection-point/">The Inflection Point of the Exponential Function</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/federated-learning-fedavg/">Federated Learning: FedAvg (Part 1)</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"GeorgeRPu/georgerpu.github.io","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 George Pu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.3/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>